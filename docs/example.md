## RAG의 모든 것

### 1. AI와 언어모델 기초 이해

#### 1.1 NLP, Natural Language Processing(자연어 처리)란 무엇인가?

NLP란 인간 언어(자연어)를 **컴퓨터가 이해하고 활용할 수 있도 만드는 기술**

- NLP의 두 가지 구성 요소
  - **NLU, Natural Language Understanding(이해): 문장의 의미를 해석**
  - **NLG, Natural Language Generation(생성): 컴퓨터가 새로운 문장을 생성**
- 예시: 사용자가 `"날씨 어때?"`라고 물으면
  - NLU는 `"날씨에 대한 질문"`임을 파악
  - NLG는 `"서울의 날씨는 맑고 28도 입니다"`라는 문장을 생성
- NLP vs 음성 인식
  - NLP는 "텍스트"를 중심으로 작동
  - 음성 인식은 "소리 -> 텍스트"로 변환하는 별도의 기술(Speech Recognition)

---

#### 1.2 NLP의 역사적 변천: 빈도수 기반에서 임베딩까지

현재의 딥러닝 기반 NLP를 이해하기 위해서는 과거의 전통적 방법론을 먼저 이해하는 것이 중요

##### 1.2.1 전통적인 빈도수 기반 모델(2010년대 이전)

딥러닝 이전, 텍스트를 숫자로 표현하는 주요 방법론은 단어의 빈도수와 통계적 중요도 기반

**1) BoW(Bag-of-Words)**

- 문장에서 단어의 **순서들은 고려하지 않고, 각 단어의 출현 빈도만 기록**하여 벡터 생성

- ```tex
  1. 각 단어에 고유한 정수 인덱스를 부여 -> 단어의 집합을 생성
  2. 각 인덱스에 단어 토큰의 등장 횟수를 기록한 벡터를 생성
  ```

- 예시: "나는 사과를 좋아하고 너는 사과를 싫어한다."

  - ```tex
    1. ["나", "는", "사과", "를", "좋아하고", "너", "는", "사과", "를", "싫어한다"]
    2. 0: 나, 1: 는, 2: 사과, 3: 를, 4: 좋아하고...
    3. [0, 1, 2, 3, 4, 5, 1, 2, 3, 6]
    4. 나(1), 는(2), 사과(2), 를(2)...
    ```

**2) TF-IDF(Term Frequency-Inverse Document Frequency)**

- **TF(단어 빈도)**: 한 문서에서 **특정 단어가 얼마나 자주 등장하는지**

- **IDF(역문서 빈도)**: 전체 문서 집합에서 **해당 특정 단어가 얼마나 잘 안나오는가**

- **목표**: 단순 빈도뿐 아니라, 문서 분류나 정보 검색 시 해당 단어의 중요도를 측정

  - '이', '가', '는' 같은 흔한 불용어의 가중치는 낮추고, 핵심 키워드의 가중치는 높임

- 한 문서에서 자주 나오지만 다른 문서에서는 잘 안나오는 단어일 수 록 중요도가 높다고 보는 방식

- > **발전**: TF-IDF의 통계적 중요도 측정 아이디어는 훗날 **정보 검색 시스템**의 표준이 된 **BM25(Best Match 25)** 알고리즘의 기반이 되었음.(BM25는 문서 길이 정규화 및 TF 값 조정 등을 통해 성능을 개선)

##### 1.2.2 빈도수 모델의 한계

구현이 간단하고 계산 속도가 빠르다는 장점이 있지만, 다음과 같은 치명적인 한계 존재

| 한계점                    | 설명                                                         |
| ------------------------- | ------------------------------------------------------------ |
| **의미적 관계 파악 불가** | "고양이"와 "야옹이"는 의미가 유사하지만, 빈도수 벡터에서는 완전히 독립된 다른 단어로 처리됨. 단어의 유사성을 이해하지 못함 |
| **문맥 무시**             | BoW는 단어의 순서를 무시하므로, "개가 사람을 물었다"와 "사람이 개를 물었다"를 거의 같은 문장으로 인식하기에, 순서가 중요한 문법적/의미적 차이를 잡아내지 못함 |
| **희소성 문제**           | 전체 단어 수가 늘어 벡터의 차원이 기하급수적으로 커지고 대부분이 0으로 채워지는 희소 벡터(Sparse Vector) 문제 발생해, 계산 비효율적 |

##### 1.2.3 딥러닝 시대의 시작(2010년대 중반)

이러한 한계를 극복하기 위해 등장한 것이 **딥러닝 기반의 임베딩(Word Embedding)** 기술.

**주요 임베딩 기법**

- **Word2Vec(2013)**: Google이 개발한 단어 임베딩 기법
  - CBOW(Continuous Bag of Words)와 Skip-gram 방식
  - 주변 단어를 통해 의미적 유사성을 학습
  - 예: `king - man + woman ≈ queen`(벡터 연산으로 의미 관계 표현 가능)

- **GloVe(2014)**: Stanford에서 개발한 글로벌 벡터 표현
  - 전체 말뭉치의 통계 정보를 활용한 임베딩

**임베딩의 혁신**

- **밀집 벡터(Dense Vector)** 사용: 희소 벡터 문제 해결
- **의미적 유사성** 표현: 비슷한 의미의 단어는 벡터 공간에서 가까이 위치
- **전이 학습** 가능: 대규모 말뭉치로 학습한 임베딩을 다양한 Task에 재사용

##### 1.2.4 현대 NLP: Transformer와 대규모 언어모델

2017년 **Transformer** 등장 이후, NLP는 완전히 새로운 차원으로 진화.

- **문맥 의존적 임베딩**: Word2Vec과 달리, 같은 단어라도 문맥에 따라 다른 벡터 표현
  - 예: "은행(bank)"이 "강둑"과 "금융기관" 중 어느 의미인지 문맥으로 판단
- **Self-Attention 메커니즘**: 문장 내 모든 단어 간의 관계를 동시에 파악
- **대규모 사전학습**: 방대한 텍스트로 학습 후, 특정 Task에 Fine-tuning
- **대표 모델**: BERT, GPT 계열(ChatGPT, Claude, Gemini 등)

**정리: NLP의 진화**

```
2010년 이전: 빈도수 기반(BoW, TF-IDF) + 통계 모델
    ↓
2013-2017: 임베딩 시대(Word2Vec, GloVe) + RNN/LSTM
    ↓
2017-현재: Transformer 시대(BERT, GPT 계열(ChatGPT, Claude, Gemini 등))
```

---
